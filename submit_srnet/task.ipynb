{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 딥페이크 탐지 모델 추론 - SRNet 제출용\n",
    "\n",
    "이 노트북은 SRNet 모델을 사용한 대회 제출용 추론 스크립트입니다.\n",
    "\n",
    "## 모델 정보\n",
    "- **아키텍처**: SRNet (Steganalysis-based ResNet)\n",
    "- **입력 크기**: 128x128\n",
    "- **색상 공간**: YCbCr\n",
    "- **파라미터 수**: ~4.8M\n",
    "\n",
    "## 평가 데이터 경로\n",
    "- 입력: `./data/` (이미지 및 비디오 혼합)\n",
    "- 출력: `./submission.csv`\n",
    "\n",
    "## 출력 형식\n",
    "- filename: 파일명 (확장자 포함)\n",
    "- label: 0(Real) 또는 1(Fake)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 라이브러리 설치"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ 라이브러리 설치 완료\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  DEPRECATION: Building 'docopt' using the legacy setup.py bdist_wheel mechanism, which will be removed in a future version. pip 25.3 will enforce this behaviour change. A possible replacement is to use the standardized build interface by setting the `--use-pep517` option, (possibly combined with `--no-build-isolation`), or adding a `pyproject.toml` file to the source tree of 'docopt'. Discussion can be found at https://github.com/pypa/pip/issues/6334\n"
     ]
    }
   ],
   "source": [
    "# 필요한 라이브러리 설치\n",
    "!pip install -q torch==2.7.1\n",
    "!pip install -q torchvision==0.22.1\n",
    "!pip install -q numpy==1.26.4\n",
    "!pip install -q opencv-python-headless==4.10.0.82\n",
    "!pip install -q pandas\n",
    "!pip install -q Pillow\n",
    "!pip install -q tqdm\n",
    "!pip install -q aifactory\n",
    "\n",
    "print(\"✓ 라이브러리 설치 완료\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 라이브러리 Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ 라이브러리 임포트 완료\n",
      "PyTorch version: 2.7.1+cpu\n",
      "CUDA available: False\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import os\n",
    "import glob\n",
    "import cv2\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "from tqdm.auto import tqdm\n",
    "from PIL import Image as PILImage\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision import transforms\n",
    "from torchvision.transforms import functional as TF\n",
    "\n",
    "print(\"✓ 라이브러리 임포트 완료\")\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"Device: {torch.cuda.get_device_name(0)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. SRNet 모델 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ SRNet 모델 정의 완료\n"
     ]
    }
   ],
   "source": [
    "# Squeeze-and-Excitation Block\n",
    "class SEBlock(nn.Module):\n",
    "    def __init__(self, channels, reduction=16):\n",
    "        super(SEBlock, self).__init__()\n",
    "        self.fc1 = nn.Linear(channels, channels // reduction)\n",
    "        self.fc2 = nn.Linear(channels // reduction, channels)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        batch, channels, _, _ = x.size()\n",
    "        y = F.adaptive_avg_pool2d(x, 1).view(batch, channels)\n",
    "        y = F.relu(self.fc1(y))\n",
    "        y = torch.sigmoid(self.fc2(y)).view(batch, channels, 1, 1)\n",
    "        return x * y.expand_as(x)\n",
    "\n",
    "# SRNet Block\n",
    "class SRNetBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, stride=1, use_se=True):\n",
    "        super(SRNetBlock, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=stride, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(out_channels)\n",
    "        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(out_channels)\n",
    "        \n",
    "        self.use_se = use_se\n",
    "        if use_se:\n",
    "            self.se = SEBlock(out_channels)\n",
    "        \n",
    "        self.shortcut = nn.Sequential()\n",
    "        if stride != 1 or in_channels != out_channels:\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=stride, bias=False),\n",
    "                nn.BatchNorm2d(out_channels)\n",
    "            )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        out = self.bn2(self.conv2(out))\n",
    "        \n",
    "        if self.use_se:\n",
    "            out = self.se(out)\n",
    "        \n",
    "        out += self.shortcut(x)\n",
    "        out = F.relu(out)\n",
    "        return out\n",
    "\n",
    "# SRNet 모델\n",
    "class SRNet(nn.Module):\n",
    "    def __init__(self, num_classes=2, input_channels=3):\n",
    "        super(SRNet, self).__init__()\n",
    "        \n",
    "        self.conv1 = nn.Conv2d(input_channels, 64, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(64)\n",
    "        \n",
    "        self.layer1 = self._make_layer(64, 64, num_blocks=2, stride=1, use_se=True)\n",
    "        self.layer2 = self._make_layer(64, 128, num_blocks=2, stride=2, use_se=True)\n",
    "        self.layer3 = self._make_layer(128, 256, num_blocks=2, stride=2, use_se=True)\n",
    "        self.layer4 = self._make_layer(256, 512, num_blocks=2, stride=2, use_se=True)\n",
    "        \n",
    "        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        self.fc = nn.Linear(512, num_classes)\n",
    "    \n",
    "    def _make_layer(self, in_channels, out_channels, num_blocks, stride, use_se):\n",
    "        layers = []\n",
    "        layers.append(SRNetBlock(in_channels, out_channels, stride, use_se))\n",
    "        for _ in range(1, num_blocks):\n",
    "            layers.append(SRNetBlock(out_channels, out_channels, stride=1, use_se=use_se))\n",
    "        return nn.Sequential(*layers)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.bn1(self.conv1(x)))\n",
    "        \n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "        x = self.layer4(x)\n",
    "        \n",
    "        x = self.avgpool(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "\n",
    "print(\"✓ SRNet 모델 정의 완료\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. YCbCr 변환 및 전처리 함수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ YCbCr 변환 함수 정의 완료\n"
     ]
    }
   ],
   "source": [
    "def rgb_to_ycbcr_tensor(img_tensor):\n",
    "    \"\"\"\n",
    "    RGB 텐서를 YCbCr 텐서로 변환\n",
    "    img_tensor: (C, H, W) 형태의 RGB 텐서 (0~1 범위)\n",
    "    \"\"\"\n",
    "    transform_matrix = torch.tensor([\n",
    "        [ 0.299,  0.587,  0.114],\n",
    "        [-0.169, -0.331,  0.500],\n",
    "        [ 0.500, -0.419, -0.081]\n",
    "    ], dtype=img_tensor.dtype, device=img_tensor.device)\n",
    "    \n",
    "    img_flat = img_tensor.permute(1, 2, 0).reshape(-1, 3)\n",
    "    ycbcr_flat = torch.matmul(img_flat, transform_matrix.T)\n",
    "    ycbcr_flat[:, 1:] += 0.5\n",
    "    \n",
    "    h, w = img_tensor.shape[1], img_tensor.shape[2]\n",
    "    ycbcr_tensor = ycbcr_flat.reshape(h, w, 3).permute(2, 0, 1)\n",
    "    \n",
    "    return ycbcr_tensor\n",
    "\n",
    "class YCbCrNormalize:\n",
    "    def __init__(self, mean, std):\n",
    "        self.mean = torch.tensor(mean).view(3, 1, 1)\n",
    "        self.std = torch.tensor(std).view(3, 1, 1)\n",
    "    \n",
    "    def __call__(self, ycbcr_tensor):\n",
    "        if ycbcr_tensor.is_cuda:\n",
    "            self.mean = self.mean.cuda()\n",
    "            self.std = self.std.cuda()\n",
    "        return (ycbcr_tensor - self.mean) / self.std\n",
    "\n",
    "print(\"✓ YCbCr 변환 함수 정의 완료\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. 모델 및 설정 로드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ 제출 환경 감지: ./model/srnet-steganalysis-model/ 사용\n",
      "Using device: cpu\n",
      "\n",
      "=== 모델 로딩 중 ===\n",
      "모델 경로: ./model/srnet-steganalysis-model\n",
      "\n",
      "✓ 모델 설정 로드:\n",
      "  아키텍처: SRNet\n",
      "  입력 크기: (128, 128)\n",
      "  색상 공간: YCbCr\n",
      "  라벨: ['Real', 'Fake']\n",
      "\n",
      "✓ YCbCr 통계 로드:\n",
      "  Mean: [0.25792643427848816, 0.4994657635688782, 0.5009396076202393]\n",
      "  Std: [0.27681994438171387, 0.0426921509206295, 0.03518436849117279]\n",
      "\n",
      "✓ 모델 가중치 로드 (경량화 버전)\n",
      "\n",
      "=== 모델 로드 완료 ===\n"
     ]
    }
   ],
   "source": [
    "# 모델 경로 설정 (자동 감지)\n",
    "if os.path.exists(\"./model/srnet-steganalysis-model\"):\n",
    "    MODEL_DIR = \"./model/srnet-steganalysis-model\"  # 제출 환경\n",
    "    print(\"✓ 제출 환경 감지: ./model/srnet-steganalysis-model/ 사용\")\n",
    "elif os.path.exists(\"./srnet_submission\"):\n",
    "    MODEL_DIR = \"./srnet_submission\"  # 개발 환경\n",
    "    print(\"✓ 개발 환경 감지: ./srnet_submission/ 사용\")\n",
    "else:\n",
    "    raise FileNotFoundError(\n",
    "        \"모델 디렉토리를 찾을 수 없습니다.\\n\"\n",
    "        \"먼저 Steganalysis.ipynb를 실행하여 모델을 학습하고 저장해주세요.\\n\"\n",
    "        \"모델은 ./srnet_submission/ 에 저장됩니다.\"\n",
    "    )\n",
    "\n",
    "# 디바이스 설정\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "print(f\"\\n=== 모델 로딩 중 ===\")\n",
    "print(f\"모델 경로: {MODEL_DIR}\\n\")\n",
    "\n",
    "# 1. 모델 설정 로드\n",
    "with open(os.path.join(MODEL_DIR, 'model_config.pkl'), 'rb') as f:\n",
    "    model_config = pickle.load(f)\n",
    "\n",
    "print(f\"✓ 모델 설정 로드:\")\n",
    "print(f\"  아키텍처: {model_config['architecture']}\")\n",
    "print(f\"  입력 크기: {model_config['input_size']}\")\n",
    "print(f\"  색상 공간: {model_config['color_space']}\")\n",
    "print(f\"  라벨: {model_config['labels']}\")\n",
    "\n",
    "# 2. YCbCr 통계 로드\n",
    "with open(os.path.join(MODEL_DIR, 'ycbcr_stats.pkl'), 'rb') as f:\n",
    "    ycbcr_stats = pickle.load(f)\n",
    "\n",
    "ycbcr_normalizer = YCbCrNormalize(\n",
    "    mean=ycbcr_stats['mean'],\n",
    "    std=ycbcr_stats['std']\n",
    ")\n",
    "print(f\"\\n✓ YCbCr 통계 로드:\")\n",
    "print(f\"  Mean: {ycbcr_stats['mean']}\")\n",
    "print(f\"  Std: {ycbcr_stats['std']}\")\n",
    "\n",
    "# 3. 모델 생성 및 가중치 로드\n",
    "model = SRNet(\n",
    "    num_classes=model_config['num_classes'],\n",
    "    input_channels=model_config['input_channels']\n",
    ")\n",
    "\n",
    "# 경량화된 모델 파일 사용 (state_dict만 포함)\n",
    "model_file = os.path.join(MODEL_DIR, 'srnet_model_light.pth')\n",
    "if not os.path.exists(model_file):\n",
    "    model_file = os.path.join(MODEL_DIR, 'srnet_model.pth')  # fallback\n",
    "    \n",
    "state_dict = torch.load(model_file, map_location=device, weights_only=False)\n",
    "\n",
    "# state_dict가 직접 state_dict인 경우와 체크포인트인 경우 모두 처리\n",
    "if isinstance(state_dict, dict) and 'model_state_dict' in state_dict:\n",
    "    model.load_state_dict(state_dict['model_state_dict'])\n",
    "    print(f\"\\n✓ 모델 가중치 로드 (전체 체크포인트):\")\n",
    "    print(f\"  Epoch: {state_dict.get('epoch', 'N/A')}\")\n",
    "    print(f\"  Val Accuracy: {state_dict.get('val_acc', 0)*100:.2f}%\")\n",
    "    print(f\"  Val Macro F1: {state_dict.get('val_macro_f1', 0):.4f}\")\n",
    "else:\n",
    "    model.load_state_dict(state_dict)\n",
    "    print(f\"\\n✓ 모델 가중치 로드 (경량화 버전)\")\n",
    "\n",
    "model = model.to(device)\n",
    "model.eval()\n",
    "\n",
    "# 4. 전처리 설정\n",
    "INPUT_SIZE = model_config['input_size']\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize(INPUT_SIZE),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "print(f\"\\n=== 모델 로드 완료 ===\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. 추론 함수 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ 추론 함수 정의 완료\n"
     ]
    }
   ],
   "source": [
    "def predict_image(image_path):\n",
    "    \"\"\"\n",
    "    이미지 파일에 대한 예측 수행\n",
    "    \n",
    "    Args:\n",
    "        image_path: 이미지 파일 경로\n",
    "    \n",
    "    Returns:\n",
    "        int: 예측 라벨 (0: Real, 1: Fake)\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # 이미지 로드\n",
    "        image = PILImage.open(image_path).convert('RGB')\n",
    "        \n",
    "        # 전처리: Resize + ToTensor\n",
    "        img_tensor = transform(image)  # (C, H, W), 0~1\n",
    "        \n",
    "        # YCbCr 변환\n",
    "        ycbcr_tensor = rgb_to_ycbcr_tensor(img_tensor)\n",
    "        \n",
    "        # YCbCr 정규화\n",
    "        ycbcr_tensor = ycbcr_normalizer(ycbcr_tensor)\n",
    "        \n",
    "        # 배치 차원 추가 및 디바이스 이동\n",
    "        ycbcr_tensor = ycbcr_tensor.unsqueeze(0).to(device)\n",
    "        \n",
    "        # 추론\n",
    "        with torch.no_grad():\n",
    "            outputs = model(ycbcr_tensor)\n",
    "            predicted_label = outputs.argmax(-1).item()\n",
    "        \n",
    "        return predicted_label\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {image_path}: {e}\")\n",
    "        return 0  # 오류 시 Real 반환\n",
    "\n",
    "\n",
    "def extract_video_frames(video_path, num_frames=5):\n",
    "    \"\"\"\n",
    "    비디오에서 여러 프레임 추출\n",
    "    \n",
    "    Args:\n",
    "        video_path: 비디오 파일 경로\n",
    "        num_frames: 추출할 프레임 수\n",
    "    \n",
    "    Returns:\n",
    "        list: PIL Image 리스트\n",
    "    \"\"\"\n",
    "    frames = []\n",
    "    \n",
    "    try:\n",
    "        cap = cv2.VideoCapture(str(video_path))\n",
    "        if not cap.isOpened():\n",
    "            return frames\n",
    "        \n",
    "        total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "        if total_frames <= 0:\n",
    "            cap.release()\n",
    "            return frames\n",
    "        \n",
    "        # 균등하게 프레임 인덱스 선택\n",
    "        if total_frames < num_frames:\n",
    "            indices = list(range(total_frames))\n",
    "        else:\n",
    "            indices = np.linspace(0, total_frames - 1, num_frames, dtype=int)\n",
    "        \n",
    "        for idx in indices:\n",
    "            cap.set(cv2.CAP_PROP_POS_FRAMES, idx)\n",
    "            ret, frame = cap.read()\n",
    "            \n",
    "            if ret and frame is not None:\n",
    "                frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "                pil_image = PILImage.fromarray(frame_rgb)\n",
    "                frames.append(pil_image)\n",
    "        \n",
    "        cap.release()\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error extracting frames from {video_path}: {e}\")\n",
    "    \n",
    "    return frames\n",
    "\n",
    "\n",
    "def predict_video(video_path, num_frames=5):\n",
    "    \"\"\"\n",
    "    비디오 파일에 대한 예측 수행 (여러 프레임 평균)\n",
    "    \n",
    "    Args:\n",
    "        video_path: 비디오 파일 경로\n",
    "        num_frames: 추출할 프레임 수\n",
    "    \n",
    "    Returns:\n",
    "        int: 예측 라벨 (0: Real, 1: Fake)\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # 프레임 추출\n",
    "        frames = extract_video_frames(video_path, num_frames)\n",
    "        \n",
    "        if len(frames) == 0:\n",
    "            return 0  # 프레임 추출 실패 시 Real 반환\n",
    "        \n",
    "        # 각 프레임에 대해 예측\n",
    "        predictions = []\n",
    "        \n",
    "        for frame in frames:\n",
    "            # 전처리\n",
    "            img_tensor = transform(frame)\n",
    "            \n",
    "            # YCbCr 변환\n",
    "            ycbcr_tensor = rgb_to_ycbcr_tensor(img_tensor)\n",
    "            \n",
    "            # YCbCr 정규화\n",
    "            ycbcr_tensor = ycbcr_normalizer(ycbcr_tensor)\n",
    "            \n",
    "            # 배치 차원 추가\n",
    "            ycbcr_tensor = ycbcr_tensor.unsqueeze(0).to(device)\n",
    "            \n",
    "            # 추론\n",
    "            with torch.no_grad():\n",
    "                outputs = model(ycbcr_tensor)\n",
    "                probs = torch.softmax(outputs, dim=-1)\n",
    "                fake_prob = probs[0, 1].item()  # Fake 확률\n",
    "                predictions.append(fake_prob)\n",
    "        \n",
    "        # 평균 확률로 최종 예측\n",
    "        avg_fake_prob = np.mean(predictions)\n",
    "        final_label = 1 if avg_fake_prob > 0.5 else 0\n",
    "        \n",
    "        return final_label\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {video_path}: {e}\")\n",
    "        return 0  # 오류 시 Real 반환\n",
    "\n",
    "\n",
    "print(\"✓ 추론 함수 정의 완료\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. 데이터 로드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ 데이터 경로 확인: ./data/\n",
      "\n",
      "=== 파일 스캔 시작 ===\n",
      "총 파일 수: 0\n",
      "  이미지: 0개\n",
      "  비디오: 0개\n"
     ]
    }
   ],
   "source": [
    "# 데이터 경로 설정\n",
    "DATA_PATH = './data/'\n",
    "\n",
    "# 개발 환경: data 디렉토리가 없으면 생성\n",
    "if not os.path.exists(DATA_PATH):\n",
    "    print(f\"⚠️  '{DATA_PATH}' 디렉토리가 없습니다.\")\n",
    "    os.makedirs(DATA_PATH, exist_ok=True)\n",
    "    print(f\"✓ '{DATA_PATH}' 디렉토리 생성\")\n",
    "    print(\"⚠️  테스트용 데이터를 ./data/ 에 넣어주세요.\")\n",
    "else:\n",
    "    print(f\"✓ 데이터 경로 확인: {DATA_PATH}\")\n",
    "\n",
    "# 지원 파일 확장자\n",
    "IMAGE_EXTS = ('.jpg', '.jpeg', '.png', '.bmp', '.webp')\n",
    "VIDEO_EXTS = ('.mp4', '.avi', '.mov', '.mkv', '.webm')\n",
    "\n",
    "# 파일 목록 수집\n",
    "print(\"\\n=== 파일 스캔 시작 ===\")\n",
    "all_files = []\n",
    "\n",
    "if os.path.exists(DATA_PATH):\n",
    "    for file in os.listdir(DATA_PATH):\n",
    "        file_path = os.path.join(DATA_PATH, file)\n",
    "        if os.path.isfile(file_path):\n",
    "            all_files.append(file)\n",
    "\n",
    "print(f\"총 파일 수: {len(all_files)}\")\n",
    "\n",
    "# 파일 타입별 분류\n",
    "image_files = [f for f in all_files if f.lower().endswith(IMAGE_EXTS)]\n",
    "video_files = [f for f in all_files if f.lower().endswith(VIDEO_EXTS)]\n",
    "\n",
    "print(f\"  이미지: {len(image_files)}개\")\n",
    "print(f\"  비디오: {len(video_files)}개\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. 추론 수행"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== 추론 시작 ===\n",
      "\n",
      "✓ 추론 완료: 0개 파일\n"
     ]
    }
   ],
   "source": [
    "# 추론 수행\n",
    "print(\"\\n=== 추론 시작 ===\")\n",
    "results = []\n",
    "\n",
    "# 이미지 추론\n",
    "if len(image_files) > 0:\n",
    "    print(\"\\n이미지 추론 중...\")\n",
    "    for filename in tqdm(image_files, desc=\"Images\"):\n",
    "        file_path = os.path.join(DATA_PATH, filename)\n",
    "        label = predict_image(file_path)\n",
    "        results.append({\n",
    "            'filename': filename,\n",
    "            'label': label\n",
    "        })\n",
    "\n",
    "# 비디오 추론\n",
    "if len(video_files) > 0:\n",
    "    print(\"\\n비디오 추론 중...\")\n",
    "    for filename in tqdm(video_files, desc=\"Videos\"):\n",
    "        file_path = os.path.join(DATA_PATH, filename)\n",
    "        label = predict_video(file_path, num_frames=5)\n",
    "        results.append({\n",
    "            'filename': filename,\n",
    "            'label': label\n",
    "        })\n",
    "\n",
    "print(f\"\\n✓ 추론 완료: {len(results)}개 파일\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. 결과 저장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== 추론 결과 확인 ===\n",
      "results 리스트 길이: 0\n",
      "⚠️  추론 결과가 없습니다!\n",
      "이전 셀을 먼저 실행해주세요.\n"
     ]
    }
   ],
   "source": [
    "# 결과 확인\n",
    "print(f\"\\n=== 추론 결과 확인 ===\")\n",
    "print(f\"results 리스트 길이: {len(results)}\")\n",
    "\n",
    "if len(results) == 0:\n",
    "    print(\"⚠️  추론 결과가 없습니다!\")\n",
    "    print(\"이전 셀을 먼저 실행해주세요.\")\n",
    "    submission_df = pd.DataFrame(columns=['filename', 'label'])\n",
    "else:\n",
    "    # DataFrame 생성\n",
    "    submission_df = pd.DataFrame(results)\n",
    "    \n",
    "    # 결과 확인\n",
    "    print(\"\\n=== 예측 결과 ===\")\n",
    "    print(submission_df.head(10))\n",
    "    print(f\"\\n라벨 분포:\")\n",
    "    print(submission_df['label'].value_counts().sort_index())\n",
    "    print(f\"  Real (0): {(submission_df['label']==0).sum()}개\")\n",
    "    print(f\"  Fake (1): {(submission_df['label']==1).sum()}개\")\n",
    "    \n",
    "    # CSV 저장\n",
    "    OUTPUT_PATH = \"./submission.csv\"\n",
    "    submission_df.to_csv(OUTPUT_PATH, index=False)\n",
    "    \n",
    "    print(f\"\\n✓ 결과 저장 완료: {OUTPUT_PATH}\")\n",
    "    print(f\"  총 {len(submission_df)}개 예측\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. 모델 제출"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "file : task\n",
      "jupyter notebook\n",
      "중계 서버 오류 : uploadCompleted\n",
      "\n",
      "✓ 제출 완료!\n"
     ]
    }
   ],
   "source": [
    "# AIFactory 제출 (원래 방식)\n",
    "import aifactory.score as aif\n",
    "\n",
    "COMPETITION_KEY = \"7ad25b19-4651-4fc9-b7b3-126ca1f23876\"\n",
    "            \n",
    "aif.submit(\n",
    "    model_name=\"deepfake_srn_model\",  # ← 대회 측이 요구한 정확한 model_name 사용\n",
    "    key=COMPETITION_KEY\n",
    ")\n",
    "\n",
    "print(\"\\n✓ 제출 완료!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
